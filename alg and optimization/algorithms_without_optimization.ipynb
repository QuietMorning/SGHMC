{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc(U_grad, theta_init, M, C, V_hat, epsilon, T, m):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Hamiltonian Monte Carlo Sampling when the Gradient Function is Known.\n",
    "    Based on Chen, Tianqi, Emily Fox, and Carlos Guestrin (2014)\n",
    "    --------------------\n",
    "    \n",
    "    Dimensions\n",
    "    -----------\n",
    "    d: number of parameters\n",
    "    T: length of samples\n",
    "    \n",
    "    Input\n",
    "    ------\n",
    "    U_grad: callable \n",
    "        Stochastic gradient estimates of posterior density with respect to distribution parameters \n",
    "        'U_grad_tilde(D, logp_data_grad, logp_prior_grad, mb_size, theta)' when the gradient is unknown\n",
    "    \n",
    "    theta_init: d-by-1 np array\n",
    "        The inital sampling point\n",
    "        \n",
    "    M: d-by-d np array\n",
    "        A mass matrix\n",
    "    \n",
    "    C: d-by-d np array\n",
    "        A user specified friction term, should be greater than B_hat = 0.5*epsilon*V_hat \n",
    "        in the sense of in the sense of positive semi-definiteness\n",
    "    \n",
    "    V_hat: d-by-d np array\n",
    "        Empirical Fisher information of theta\n",
    "        \n",
    "    epsilon: float\n",
    "        Step size\n",
    "    \n",
    "    T: int\n",
    "        Number of samples drawn from the desired distribution\n",
    "        \n",
    "    m: int\n",
    "        Number of steps for each draw\n",
    "    \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    theta_s: T-by-d np array\n",
    "        Draws sampled from the desired distrition\n",
    "    \n",
    "    r_s: T-by-d np array\n",
    "        Draws of the momentun variables\n",
    "    \n",
    "    \"\"\"\n",
    "    d = len(theta_init)\n",
    "    theta_s = np.zeros((T, d))\n",
    "    r_s = np.zeros((T, d))\n",
    "    theta_s[0] = theta_init\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    B_hat = 0.5*epsilon*V_hat\n",
    "    \n",
    "    if d > 1:\n",
    "        sd = np.linalg.cholesky(2*epsilon*(C-B_hat))\n",
    "        r_s = np.random.multivariate_normal(np.zeros(d),M, size = T)\n",
    "    elif d==1:\n",
    "        sd = np.sqrt(2*epsilon*(C-B_hat))\n",
    "        r_s = np.sqrt(M)*np.random.randn(T).reshape(T,1)\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        theta0 = theta_s[t]\n",
    "        r0 = r_s[t]\n",
    "        for i in range(m):\n",
    "            theta0 = theta0 + epsilon*np.dot(M_inv,r0)\n",
    "            r0 = r0 - epsilon*U_grad(theta0) - epsilon*np.dot(np.dot(C,M_inv),r0) +  np.dot(sd,np.random.randn(d))\n",
    "        theta_s[t+1] = theta0\n",
    "    \n",
    "    return [theta_s,r_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_grad_tilde(theta, data, logp_data_grad, logp_prior_grad, mb_size):\n",
    "    \"\"\"\n",
    "    Stochastic gradient estimates of posterior density with respect to distribution parameters\n",
    "    Based on a minibatch D_hat sampled uniformly at random from D\n",
    "    ------------------------\n",
    "    \n",
    "    Dimensions\n",
    "    -----------\n",
    "    n: number of observations from the data\n",
    "    m: dimension of the data\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    D: n-by-m np array\n",
    "        Dataset\n",
    "        \n",
    "    logp_data_grad: callable 'logp_data_grad(data, theta)'\n",
    "        Gradient of likelihood of the data with respect to distribution parameters\n",
    "    \n",
    "    logp_prior_grad: callable 'logp_prior_grad(theta)'\n",
    "        Gradient of prior with respect to distribution parameters\n",
    "    \n",
    "    mb_size: int\n",
    "        Size of the minibatch\n",
    "    \n",
    "    theta: d-by-1 np array\n",
    "        Distribution parameters\n",
    "    \n",
    "    Output\n",
    "    -----\n",
    "    U_tilde: d-by-1 np array\n",
    "        Stochastic gradient estimates of posterior density with respect to distribution parameters\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    data_hat = data[np.random.choice(range(n), size = mb_size, replace = False)]\n",
    "    U_tilde = -(n/mb_size)*logp_data_grad(data_hat, theta) - logp_prior_grad(theta)\n",
    "    return U_tilde\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_naive(U_grad, U, theta_init, M, V_hat, epsilon, T, m, MH=True, resample=False):\n",
    "    \"\"\"\n",
    "    Naive Stochastic Gradient Hamiltonian Monte Carlo Sampling.\n",
    "    --------------------\n",
    "    \n",
    "    Input\n",
    "    ------\n",
    "    U_grad: callable \n",
    "        Stochastic gradient estimates of posterior density with respect to distribution parameters \n",
    "        'U_grad_tilde(D, logp_data_grad, logp_prior_grad, mb_size, theta)' when the gradient is unknown\n",
    "    \n",
    "    U: callable\n",
    "        Potential energy function\n",
    "    \n",
    "    theta_init: d-by-1 np array\n",
    "        The inital sampling point\n",
    "        \n",
    "    M: d-by-d np array\n",
    "        A mass matrix\n",
    "    \n",
    "    V_hat: d-by-d np array\n",
    "        Empirical Fisher information of theta\n",
    "        \n",
    "    epsilon: float\n",
    "        Step size\n",
    "    \n",
    "    T: int\n",
    "        Number of samples drawn from the desired distribution\n",
    "        \n",
    "    m: int\n",
    "        Number of steps for each draw\n",
    "    \n",
    "    MH: Boolean\n",
    "        Whether to incorporate a Metropolis-Hasting correction\n",
    "    \n",
    "    resample: Boolean\n",
    "        Whether to resample the momentum variables \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    theta_s: T-by-d np array\n",
    "        Draws sampled from the desired distrition\n",
    "    \n",
    "    r_s: T-by-d np array\n",
    "        Draws of the momentun variables\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    d = len(theta_init)\n",
    "    theta_s = np.zeros((T, d))\n",
    "    theta_s[0] = theta_init\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    B_hat = 0.5*epsilon*V_hat\n",
    "    \n",
    "    if d > 1:\n",
    "        sd = np.linalg.cholesky(2*epsilon*B_hat)\n",
    "        r_s = np.random.multivariate_normal(np.zeros(d),M, size = T)\n",
    "    elif d==1:\n",
    "        sd = np.sqrt(2*epsilon*B_hat)\n",
    "        r_s = np.sqrt(M)*np.random.randn(T).reshape(T,1)\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        theta0 = theta_s[t]\n",
    "        r0 = r_s[t]\n",
    "        \n",
    "        for i in range(m):\n",
    "            theta0 = theta0 + epsilon*np.dot(M_inv,r0)\n",
    "            r0 = r0 - epsilon*U_grad(theta0) + np.dot(sd,np.random.randn(d))\n",
    "               \n",
    "        ## M-H correction \n",
    "        if MH==True:\n",
    "            u = np.random.rand(1)\n",
    "            H1 = U(theta_s[t]) + 0.5*np.dot(np.dot(r_s[t],M_inv),r_s[t])\n",
    "            H2 = U(theta0) + 0.5*np.dot(np.dot(r0,M_inv),r0)\n",
    "            rho = np.exp(H1-H2)\n",
    "            if u < np.minimum(1,rho):\n",
    "                theta_s[t+1] = theta0\n",
    "                if resample==False: \n",
    "                    r_s[t+1] = r0\n",
    "            else:\n",
    "                theta_s[t+1] = theta_s[t]\n",
    "                if resample==False: \n",
    "                    r_s[t+1] = r_s[t]\n",
    "        else:\n",
    "            theta_s[t+1] = theta0\n",
    "            if resample==False:\n",
    "                r_s[t+1] = r0\n",
    "    \n",
    "    return [theta_s,r_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmc(U_grad, U, theta_init, M, epsilon, T, m, MH=True, resample=False):\n",
    "    \"\"\"\n",
    "    Hamiltonian Monte Carlo Sampling.\n",
    "    --------------------\n",
    "    \n",
    "    Input\n",
    "    ------\n",
    "    U_grad: callable \n",
    "        Gradient of U with respect to distribution parameters \n",
    "    \n",
    "    U: callable\n",
    "        Potential energy function\n",
    "    \n",
    "    theta_init: d-by-1 np array\n",
    "        The inital sampling point\n",
    "        \n",
    "    M: d-by-d np array\n",
    "        A mass matrix\n",
    "    \n",
    "    epsilon: float\n",
    "        Step size\n",
    "    \n",
    "    T: int\n",
    "        Number of samples drawn from the desired distribution\n",
    "        \n",
    "    m: int\n",
    "        Number of steps for each draw\n",
    "    \n",
    "    MH: Boolean\n",
    "        Whether to incorporate a Metropolis-Hasting correction\n",
    "    \n",
    "    resample: Boolean\n",
    "        Whether to resample the momentum variables \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    theta_s: T-by-d np array\n",
    "        Draws sampled from the desired distrition\n",
    "    \n",
    "    r_s: T-by-d np array\n",
    "        Draws of the momentun variables\n",
    "    \n",
    "    \"\"\"\n",
    "    d = len(theta_init) \n",
    "    theta_s = np.zeros((T, d))\n",
    "    theta_s[0] = theta_init\n",
    "    M_inv = np.linalg.inv(M)\n",
    "\n",
    "    if d > 1:\n",
    "        r_s = np.random.multivariate_normal(np.zeros(d),M, size=T)\n",
    "    elif d==1:\n",
    "        r_s = np.sqrt(M)*np.random.randn(T).reshape(T,1)\n",
    "        \n",
    "    for t in range(T-1):\n",
    "\n",
    "        theta0 = theta_s[t]\n",
    "        r0 = r_s[t]\n",
    "        \n",
    "        ## leapfrog\n",
    "        r0 = r0 - 0.5*epsilon*U_grad(theta0)\n",
    "        for i in range(m-1):\n",
    "            theta0 = theta0 + epsilon*np.dot(M_inv,r0)\n",
    "            r0 = r0 - epsilon*U_grad(theta0)\n",
    "        theta0 = theta0 + epsilon*np.dot(M_inv,r0)\n",
    "        r0 = r0 - 0.5*epsilon*U_grad(theta0) \n",
    "        \n",
    "        ## M-H correction \n",
    "        if MH==True:\n",
    "            u = np.random.rand(1)\n",
    "            H1 = U(theta_s[t]) + 0.5*np.dot(np.dot(r_s[t],M_inv),r_s[t])\n",
    "            H2 = U(theta0) + 0.5*np.dot(np.dot(r0,M_inv),r0)\n",
    "            rho = np.exp(H1-H2)\n",
    "            if u < np.minimum(1,rho):\n",
    "                theta_s[t+1] = theta0\n",
    "                if resample==False: \n",
    "                    r_s[t+1] = r0\n",
    "            else:\n",
    "                theta_s[t+1] = theta_s[t]\n",
    "                if resample==False: \n",
    "                    r_s[t+1] = r_s[t]\n",
    "        else:\n",
    "            theta_s[t+1] = theta0\n",
    "            if resample==False:\n",
    "                r_s[t+1] = r0\n",
    "\n",
    "    return [theta_s,r_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_with_data(data, logp_data_grad, logp_prior_grad, mb_size, theta_init, M, C, V_hat, epsilon, T, m):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Hamiltonian Monte Carlo Sampling when the Gradient Function is Unknown.\n",
    "    Based on Chen, Tianqi, Emily Fox, and Carlos Guestrin (2014) \n",
    "    ----------------------------\n",
    "    \n",
    "    Dimensions: \n",
    "    -----------\n",
    "    d: number of parameters \n",
    "    T: length of samples \n",
    "    p: dimension of the data\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    data: T-by-p np array\n",
    "        Training data. \n",
    "    \n",
    "    logp_data_grad: callable function \n",
    "        Calculate the gradient of log likelihood function of data given the parameters. \n",
    "    \n",
    "    logp_prior_grad: callable function \n",
    "        Calculate the gradient of the log prior. \n",
    "    \n",
    "    mb_size: int, \n",
    "        mini batch size. \n",
    "    \n",
    "    theta_init: d-by-1 np array\n",
    "        The inital sampling point\n",
    "        \n",
    "    M: d-by-d np array\n",
    "        A mass matrix\n",
    "    \n",
    "    C: d-by-d np array\n",
    "        A user specified friction term, should be greater than B_hat = 0.5*epsilon*V_hat \n",
    "        in the sense of in the sense of positive semi-definiteness\n",
    "    \n",
    "    V_hat: d-by-d np array\n",
    "        Empirical Fisher information of theta\n",
    "        \n",
    "    epsilon: float\n",
    "        Step size\n",
    "    \n",
    "    T: int\n",
    "        Number of samples drawn from the desired distribution\n",
    "        \n",
    "    m: int\n",
    "        Number of steps for each draw\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    theta_s: T-by-d np array\n",
    "        Draws sampled from the desired distrition\n",
    "    \n",
    "    r_s: T-by-d np array\n",
    "        Draws of the momentun variables\n",
    "    \n",
    "    \"\"\"\n",
    "    d = theta_init.shape[0]\n",
    "    theta_s = np.zeros((T, d))\n",
    "    r_s = np.zeros((T, d))\n",
    "    theta_s[0] = theta_init\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    B_hat = 0.5*epsilon*V_hat\n",
    "    \n",
    "    \n",
    "    n = data.shape[0]\n",
    "    data_hat = data[np.random.choice(range(n), size = mb_size, replace = False)]\n",
    "    \n",
    "    if d > 1:\n",
    "        sd = np.linalg.cholesky(2*epsilon*(C-B_hat))\n",
    "        r_s = np.random.multivariate_normal(np.zeros(d),M, size = T)\n",
    "    elif d==1:\n",
    "        sd = np.sqrt(2*epsilon*(C-B_hat))\n",
    "        r_s = np.sqrt(M)*np.random.randn(T).reshape(T,1)\n",
    "\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        theta0 = theta_s[t]\n",
    "        r0 = r_s[t]\n",
    "        for i in range(m):\n",
    "            theta0 = theta0 + epsilon*M_inv@r0\n",
    "            r0 = r0 + epsilon*((n/mb_size)*logp_data_grad(data_hat, theta0) +logp_prior_grad(theta0)) - epsilon*C@M_inv@r0 + sd@np.random.randn(d)\n",
    "        theta_s[t+1] = theta0\n",
    "    \n",
    "    return [theta_s,r_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc_with_grad(U_grad, theta_init, M, C, V_hat, epsilon, T, m):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Hamiltonian Monte Carlo Sampling.\n",
    "    Based on Chen, Tianqi, Emily Fox, and Carlos Guestrin (2014)\n",
    "    --------------------\n",
    "    \n",
    "    Dimensions\n",
    "    -----------\n",
    "    d: number of parameters\n",
    "    T: length of samples\n",
    "    \n",
    "    Input\n",
    "    ------\n",
    "    U_grad: callable \n",
    "        Stochastic gradient estimates of posterior density with respect to distribution parameters \n",
    "        'U_grad_tilde(D, logp_data_grad, logp_prior_grad, mb_size, theta)' when the gradient is unknown\n",
    "    \n",
    "    theta_init: d-by-1 np array\n",
    "        The inital sampling point\n",
    "        \n",
    "    M: d-by-d np array\n",
    "        A mass matrix\n",
    "    \n",
    "    C: d-by-d np array\n",
    "        A user specified friction term, should be greater than B_hat = 0.5*epsilon*V_hat \n",
    "        in the sense of in the sense of positive semi-definiteness\n",
    "    \n",
    "    V_hat: d-by-d np array\n",
    "        Empirical Fisher information of theta\n",
    "        \n",
    "    epsilon: float\n",
    "        Step size\n",
    "    \n",
    "    T: int\n",
    "        Number of samples drawn from the desired distribution\n",
    "        \n",
    "    m: int\n",
    "        Number of steps for each draw\n",
    "    \n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    theta_s: T-by-d np array\n",
    "        Draws sampled from the desired distrition\n",
    "    \n",
    "    r_s: T-by-d np array\n",
    "        Draws of the momentun variables\n",
    "    \n",
    "    \"\"\"\n",
    "    d = len(theta_init)\n",
    "    theta_s = np.zeros((T, d))\n",
    "    r_s = np.zeros((T, d))\n",
    "    theta_s[0] = theta_init\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    B_hat = 0.5*epsilon*V_hat\n",
    "    \n",
    "    if d > 1:\n",
    "        sd = np.linalg.cholesky(2*epsilon*(C-B_hat))\n",
    "        r_s = np.random.multivariate_normal(np.zeros(d),M, size = T)\n",
    "    elif d==1:\n",
    "        sd = np.sqrt(2*epsilon*(C-B_hat))\n",
    "        r_s = np.sqrt(M)*np.random.randn(T).reshape(T,1)\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        theta0 = theta_s[t]\n",
    "        r0 = r_s[t]\n",
    "        for i in range(m):\n",
    "            theta0 = theta0 + epsilon*np.dot(M_inv,r0)\n",
    "            r0 = r0 - epsilon*U_grad(theta0) - epsilon*np.dot(np.dot(C,M_inv),r0) +  np.dot(sd,np.random.randn(d))\n",
    "        theta_s[t+1] = theta0\n",
    "    \n",
    "    return [theta_s,r_s]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
